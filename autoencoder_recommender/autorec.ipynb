{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tempfile import TemporaryDirectory\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from utils import *\n",
    "import time\n",
    "from model import AutoRecItemBased\n",
    "from torch.optim import lr_scheduler\n",
    "from train import train_model_item_based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users: 943, number of items: 1682\n",
      "matrix sparsity: 0.936953\n",
      "   user_id  item_id  rating  timestamp\n",
      "0      196      242       3  881250949\n",
      "1      186      302       3  891717742\n",
      "2       22      377       1  878887116\n",
      "3      244       51       2  880606923\n",
      "4      166      346       1  886397596\n",
      "5      298      474       4  884182806\n",
      "6      115      265       2  881171488\n",
      "7      253      465       5  891628467\n",
      "8      305      451       3  886324817\n",
      "9        6       86       3  883603013\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data/ml-100k\"\n",
    "data, num_users, num_items = read_data_ml100k(DATA_DIR)\n",
    "sparsity:int = 1 - len(data) / (num_users * num_items)\n",
    "print(f'number of users: {num_users}, number of items: {num_items}')\n",
    "print(f'matrix sparsity: {sparsity:f}')\n",
    "print(data.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of train data: 89836\n",
      "Length of test data 10164\n",
      "user interaction matrix is a 2D matrix where rows represent items and columns represent users and the value of each element is the ranking.\n",
      "\n",
      "user-item interaction matrix training set shape: (1682, 943)\n",
      "user-item interaction matrix training set:\n",
      " [[0. 4. 0. ... 5. 0. 0.]\n",
      " [3. 0. 0. ... 0. 0. 5.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "user-item interaction matrix test set shape: (1682, 943)\n",
      "user-item interaction matrix test set:\n",
      " [[5. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_data, test_data = split_data_ml100k(data, num_users)\n",
    "print(\"Lenght of train data:\",len(train_data))\n",
    "print(\"Length of test data\",len(test_data))\n",
    "\n",
    "# Convert into user-item interaction matrix\n",
    "users_train, items_train, _, train_inter_mat = load_data_ml100k(train_data, num_users,num_items)\n",
    "print(\"user interaction matrix is a 2D matrix where rows represent items and columns represent users and the value of each element is the ranking.\")\n",
    "print()\n",
    "print(\"user-item interaction matrix training set shape:\",train_inter_mat.shape)\n",
    "print(\"user-item interaction matrix training set:\\n\",train_inter_mat)\n",
    "\n",
    "users_test, items_test, _, test_inter_mat = load_data_ml100k(test_data, num_users,num_items)\n",
    "print(\"user-item interaction matrix test set shape:\",test_inter_mat.shape)\n",
    "print(\"user-item interaction matrix test set:\\n\",test_inter_mat)\n",
    "# Convert to PyTorch tensors\n",
    "train_inter_mat_tensor = torch.tensor(train_inter_mat, dtype=torch.float32)\n",
    "test_inter_mat_tensor = torch.tensor(test_inter_mat, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset\n",
    "train_inter_dataset = TensorDataset(train_inter_mat_tensor)\n",
    "test_inter_dataset = TensorDataset(test_inter_mat_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "train_iter = DataLoader(train_inter_dataset, shuffle=True, batch_size=1024)\n",
    "test_iter = DataLoader(test_inter_dataset, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/149\n",
      "----------\n",
      "train Loss: 6703.3252\n",
      "val Loss: 74.7427\n",
      "\n",
      "Epoch 1/149\n",
      "----------\n",
      "train Loss: 602.2306\n",
      "val Loss: 78.0398\n",
      "\n",
      "Epoch 2/149\n",
      "----------\n",
      "train Loss: 694.0230\n",
      "val Loss: 76.1651\n",
      "\n",
      "Epoch 3/149\n",
      "----------\n",
      "train Loss: 645.7611\n",
      "val Loss: 70.3653\n",
      "\n",
      "Epoch 4/149\n",
      "----------\n",
      "train Loss: 596.4939\n",
      "val Loss: 63.2968\n",
      "\n",
      "Epoch 5/149\n",
      "----------\n",
      "train Loss: 399.2287\n",
      "val Loss: 54.0151\n",
      "\n",
      "Epoch 6/149\n",
      "----------\n",
      "train Loss: 226.4143\n",
      "val Loss: 54.6155\n",
      "\n",
      "Epoch 7/149\n",
      "----------\n",
      "train Loss: 211.2130\n",
      "val Loss: 49.3306\n",
      "\n",
      "Epoch 8/149\n",
      "----------\n",
      "train Loss: 206.2715\n",
      "val Loss: 49.0949\n",
      "\n",
      "Epoch 9/149\n",
      "----------\n",
      "train Loss: 175.0069\n",
      "val Loss: 45.7979\n",
      "\n",
      "Epoch 10/149\n",
      "----------\n",
      "train Loss: 154.2676\n",
      "val Loss: 44.0448\n",
      "\n",
      "Epoch 11/149\n",
      "----------\n",
      "train Loss: 130.8462\n",
      "val Loss: 43.4369\n",
      "\n",
      "Epoch 12/149\n",
      "----------\n",
      "train Loss: 113.6371\n",
      "val Loss: 41.2465\n",
      "\n",
      "Epoch 13/149\n",
      "----------\n",
      "train Loss: 112.8900\n",
      "val Loss: 40.8549\n",
      "\n",
      "Epoch 14/149\n",
      "----------\n",
      "train Loss: 103.1138\n",
      "val Loss: 40.1407\n",
      "\n",
      "Epoch 15/149\n",
      "----------\n",
      "train Loss: 95.4703\n",
      "val Loss: 38.4229\n",
      "\n",
      "Epoch 16/149\n",
      "----------\n",
      "train Loss: 85.4757\n",
      "val Loss: 38.7721\n",
      "\n",
      "Epoch 17/149\n",
      "----------\n",
      "train Loss: 82.3295\n",
      "val Loss: 37.1360\n",
      "\n",
      "Epoch 18/149\n",
      "----------\n",
      "train Loss: 75.3270\n",
      "val Loss: 37.2132\n",
      "\n",
      "Epoch 19/149\n",
      "----------\n",
      "train Loss: 73.0386\n",
      "val Loss: 36.4960\n",
      "\n",
      "Epoch 20/149\n",
      "----------\n",
      "train Loss: 70.4844\n",
      "val Loss: 35.7145\n",
      "\n",
      "Epoch 21/149\n",
      "----------\n",
      "train Loss: 68.1648\n",
      "val Loss: 35.2125\n",
      "\n",
      "Epoch 22/149\n",
      "----------\n",
      "train Loss: 67.6157\n",
      "val Loss: 34.5133\n",
      "\n",
      "Epoch 23/149\n",
      "----------\n",
      "train Loss: 66.3844\n",
      "val Loss: 34.1526\n",
      "\n",
      "Epoch 24/149\n",
      "----------\n",
      "train Loss: 64.7444\n",
      "val Loss: 34.1807\n",
      "\n",
      "Epoch 25/149\n",
      "----------\n",
      "train Loss: 60.6412\n",
      "val Loss: 33.0640\n",
      "\n",
      "Epoch 26/149\n",
      "----------\n",
      "train Loss: 59.2430\n",
      "val Loss: 33.2681\n",
      "\n",
      "Epoch 27/149\n",
      "----------\n",
      "train Loss: 58.0138\n",
      "val Loss: 32.5085\n",
      "\n",
      "Epoch 28/149\n",
      "----------\n",
      "train Loss: 59.4623\n",
      "val Loss: 32.6259\n",
      "\n",
      "Epoch 29/149\n",
      "----------\n",
      "train Loss: 56.1864\n",
      "val Loss: 32.1673\n",
      "\n",
      "Epoch 30/149\n",
      "----------\n",
      "train Loss: 58.2651\n",
      "val Loss: 31.7849\n",
      "\n",
      "Epoch 31/149\n",
      "----------\n",
      "train Loss: 55.6851\n",
      "val Loss: 31.4947\n",
      "\n",
      "Epoch 32/149\n",
      "----------\n",
      "train Loss: 56.3879\n",
      "val Loss: 31.2235\n",
      "\n",
      "Epoch 33/149\n",
      "----------\n",
      "train Loss: 54.4738\n",
      "val Loss: 31.3841\n",
      "\n",
      "Epoch 34/149\n",
      "----------\n",
      "train Loss: 53.3469\n",
      "val Loss: 30.9042\n",
      "\n",
      "Epoch 35/149\n",
      "----------\n",
      "train Loss: 55.4781\n",
      "val Loss: 30.9461\n",
      "\n",
      "Epoch 36/149\n",
      "----------\n",
      "train Loss: 58.7718\n",
      "val Loss: 30.3754\n",
      "\n",
      "Epoch 37/149\n",
      "----------\n",
      "train Loss: 58.2324\n",
      "val Loss: 30.4520\n",
      "\n",
      "Epoch 38/149\n",
      "----------\n",
      "train Loss: 52.4196\n",
      "val Loss: 30.0140\n",
      "\n",
      "Epoch 39/149\n",
      "----------\n",
      "train Loss: 52.4601\n",
      "val Loss: 30.2520\n",
      "\n",
      "Epoch 40/149\n",
      "----------\n",
      "train Loss: 50.4283\n",
      "val Loss: 29.3727\n",
      "\n",
      "Epoch 41/149\n",
      "----------\n",
      "train Loss: 54.9311\n",
      "val Loss: 30.1402\n",
      "\n",
      "Epoch 42/149\n",
      "----------\n",
      "train Loss: 60.1457\n",
      "val Loss: 28.4928\n",
      "\n",
      "Epoch 43/149\n",
      "----------\n",
      "train Loss: 59.7319\n",
      "val Loss: 31.0055\n",
      "\n",
      "Epoch 44/149\n",
      "----------\n",
      "train Loss: 67.1280\n",
      "val Loss: 26.7835\n",
      "\n",
      "Epoch 45/149\n",
      "----------\n",
      "train Loss: 78.7459\n",
      "val Loss: 33.0502\n",
      "\n",
      "Epoch 46/149\n",
      "----------\n",
      "train Loss: 96.7342\n",
      "val Loss: 26.8863\n",
      "\n",
      "Epoch 47/149\n",
      "----------\n",
      "train Loss: 79.5731\n",
      "val Loss: 29.4552\n",
      "\n",
      "Epoch 48/149\n",
      "----------\n",
      "train Loss: 64.1662\n",
      "val Loss: 30.4649\n",
      "\n",
      "Epoch 49/149\n",
      "----------\n",
      "train Loss: 68.2336\n",
      "val Loss: 26.0566\n",
      "\n",
      "Epoch 50/149\n",
      "----------\n",
      "train Loss: 75.6518\n",
      "val Loss: 26.0567\n",
      "\n",
      "Epoch 51/149\n",
      "----------\n",
      "train Loss: 77.5056\n",
      "val Loss: 26.0568\n",
      "\n",
      "Epoch 52/149\n",
      "----------\n",
      "train Loss: 77.1233\n",
      "val Loss: 26.0571\n",
      "\n",
      "Epoch 53/149\n",
      "----------\n",
      "train Loss: 74.5989\n",
      "val Loss: 26.0574\n",
      "\n",
      "Epoch 54/149\n",
      "----------\n",
      "train Loss: 77.3630\n",
      "val Loss: 26.0577\n",
      "\n",
      "Epoch 55/149\n",
      "----------\n",
      "train Loss: 74.7700\n",
      "val Loss: 26.0580\n",
      "\n",
      "Epoch 56/149\n",
      "----------\n",
      "train Loss: 78.7638\n",
      "val Loss: 26.0584\n",
      "\n",
      "Epoch 57/149\n",
      "----------\n",
      "train Loss: 76.9042\n",
      "val Loss: 26.0588\n",
      "\n",
      "Epoch 58/149\n",
      "----------\n",
      "train Loss: 77.5920\n",
      "val Loss: 26.0592\n",
      "\n",
      "Epoch 59/149\n",
      "----------\n",
      "train Loss: 75.7500\n",
      "val Loss: 26.0596\n",
      "\n",
      "Epoch 60/149\n",
      "----------\n",
      "train Loss: 75.9492\n",
      "val Loss: 26.0600\n",
      "\n",
      "Epoch 61/149\n",
      "----------\n",
      "train Loss: 75.5021\n",
      "val Loss: 26.0604\n",
      "\n",
      "Epoch 62/149\n",
      "----------\n",
      "train Loss: 73.4895\n",
      "val Loss: 26.0609\n",
      "\n",
      "Epoch 63/149\n",
      "----------\n",
      "train Loss: 77.4374\n",
      "val Loss: 26.0613\n",
      "\n",
      "Epoch 64/149\n",
      "----------\n",
      "train Loss: 75.9439\n",
      "val Loss: 26.0617\n",
      "\n",
      "Epoch 65/149\n",
      "----------\n",
      "train Loss: 74.7342\n",
      "val Loss: 26.0622\n",
      "\n",
      "Epoch 66/149\n",
      "----------\n",
      "train Loss: 75.5064\n",
      "val Loss: 26.0626\n",
      "\n",
      "Epoch 67/149\n",
      "----------\n",
      "train Loss: 75.0710\n",
      "val Loss: 26.0630\n",
      "\n",
      "Epoch 68/149\n",
      "----------\n",
      "train Loss: 73.6579\n",
      "val Loss: 26.0634\n",
      "\n",
      "Epoch 69/149\n",
      "----------\n",
      "train Loss: 74.8076\n",
      "val Loss: 26.0638\n",
      "\n",
      "Epoch 70/149\n",
      "----------\n",
      "train Loss: 79.0372\n",
      "val Loss: 26.0643\n",
      "\n",
      "Epoch 71/149\n",
      "----------\n",
      "train Loss: 78.2584\n",
      "val Loss: 26.0647\n",
      "\n",
      "Epoch 72/149\n",
      "----------\n",
      "train Loss: 77.6949\n",
      "val Loss: 26.0651\n",
      "\n",
      "Epoch 73/149\n",
      "----------\n",
      "train Loss: 74.8128\n",
      "val Loss: 26.0655\n",
      "\n",
      "Epoch 74/149\n",
      "----------\n",
      "train Loss: 74.2746\n",
      "val Loss: 26.0659\n",
      "\n",
      "Epoch 75/149\n",
      "----------\n",
      "train Loss: 75.8593\n",
      "val Loss: 26.0663\n",
      "\n",
      "Epoch 76/149\n",
      "----------\n",
      "train Loss: 76.1516\n",
      "val Loss: 26.0667\n",
      "\n",
      "Epoch 77/149\n",
      "----------\n",
      "train Loss: 75.9649\n",
      "val Loss: 26.0671\n",
      "\n",
      "Epoch 78/149\n",
      "----------\n",
      "train Loss: 77.1509\n",
      "val Loss: 26.0675\n",
      "\n",
      "Epoch 79/149\n",
      "----------\n",
      "train Loss: 75.1423\n",
      "val Loss: 26.0679\n",
      "\n",
      "Epoch 80/149\n",
      "----------\n",
      "train Loss: 76.6958\n",
      "val Loss: 26.0683\n",
      "\n",
      "Epoch 81/149\n",
      "----------\n",
      "train Loss: 76.6973\n",
      "val Loss: 26.0687\n",
      "\n",
      "Epoch 82/149\n",
      "----------\n",
      "train Loss: 77.0185\n",
      "val Loss: 26.0691\n",
      "\n",
      "Epoch 83/149\n",
      "----------\n",
      "train Loss: 75.6022\n",
      "val Loss: 26.0695\n",
      "\n",
      "Epoch 84/149\n",
      "----------\n",
      "train Loss: 77.8801\n",
      "val Loss: 26.0699\n",
      "\n",
      "Epoch 85/149\n",
      "----------\n",
      "train Loss: 77.7948\n",
      "val Loss: 26.0703\n",
      "\n",
      "Epoch 86/149\n",
      "----------\n",
      "train Loss: 78.0552\n",
      "val Loss: 26.0707\n",
      "\n",
      "Epoch 87/149\n",
      "----------\n",
      "train Loss: 78.7369\n",
      "val Loss: 26.0711\n",
      "\n",
      "Epoch 88/149\n",
      "----------\n",
      "train Loss: 73.7743\n",
      "val Loss: 26.0714\n",
      "\n",
      "Epoch 89/149\n",
      "----------\n",
      "train Loss: 76.1561\n",
      "val Loss: 26.0718\n",
      "\n",
      "Epoch 90/149\n",
      "----------\n",
      "train Loss: 78.0544\n",
      "val Loss: 26.0722\n",
      "\n",
      "Epoch 91/149\n",
      "----------\n",
      "train Loss: 74.3226\n",
      "val Loss: 26.0726\n",
      "\n",
      "Epoch 92/149\n",
      "----------\n",
      "train Loss: 77.7680\n",
      "val Loss: 26.0730\n",
      "\n",
      "Epoch 93/149\n",
      "----------\n",
      "train Loss: 75.0900\n",
      "val Loss: 26.0734\n",
      "\n",
      "Epoch 94/149\n",
      "----------\n",
      "train Loss: 74.1259\n",
      "val Loss: 26.0737\n",
      "\n",
      "Epoch 95/149\n",
      "----------\n",
      "train Loss: 75.2196\n",
      "val Loss: 26.0741\n",
      "\n",
      "Epoch 96/149\n",
      "----------\n",
      "train Loss: 74.5520\n",
      "val Loss: 26.0745\n",
      "\n",
      "Epoch 97/149\n",
      "----------\n",
      "train Loss: 79.6324\n",
      "val Loss: 26.0749\n",
      "\n",
      "Epoch 98/149\n",
      "----------\n",
      "train Loss: 75.6591\n",
      "val Loss: 26.0752\n",
      "\n",
      "Epoch 99/149\n",
      "----------\n",
      "train Loss: 77.0054\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 100/149\n",
      "----------\n",
      "train Loss: 74.9663\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 101/149\n",
      "----------\n",
      "train Loss: 74.5276\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 102/149\n",
      "----------\n",
      "train Loss: 73.1622\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 103/149\n",
      "----------\n",
      "train Loss: 75.8260\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 104/149\n",
      "----------\n",
      "train Loss: 73.3196\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 105/149\n",
      "----------\n",
      "train Loss: 77.3765\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 106/149\n",
      "----------\n",
      "train Loss: 76.6422\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 107/149\n",
      "----------\n",
      "train Loss: 75.8719\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 108/149\n",
      "----------\n",
      "train Loss: 80.2065\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 109/149\n",
      "----------\n",
      "train Loss: 78.2494\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 110/149\n",
      "----------\n",
      "train Loss: 74.9175\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 111/149\n",
      "----------\n",
      "train Loss: 76.4675\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 112/149\n",
      "----------\n",
      "train Loss: 76.7561\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 113/149\n",
      "----------\n",
      "train Loss: 77.3822\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 114/149\n",
      "----------\n",
      "train Loss: 75.9627\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 115/149\n",
      "----------\n",
      "train Loss: 78.1508\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 116/149\n",
      "----------\n",
      "train Loss: 77.4032\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 117/149\n",
      "----------\n",
      "train Loss: 78.1745\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 118/149\n",
      "----------\n",
      "train Loss: 76.8683\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 119/149\n",
      "----------\n",
      "train Loss: 76.1714\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 120/149\n",
      "----------\n",
      "train Loss: 76.8154\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 121/149\n",
      "----------\n",
      "train Loss: 77.1851\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 122/149\n",
      "----------\n",
      "train Loss: 74.2714\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 123/149\n",
      "----------\n",
      "train Loss: 73.6693\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 124/149\n",
      "----------\n",
      "train Loss: 77.4266\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 125/149\n",
      "----------\n",
      "train Loss: 75.6016\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 126/149\n",
      "----------\n",
      "train Loss: 80.9548\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 127/149\n",
      "----------\n",
      "train Loss: 77.0801\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 128/149\n",
      "----------\n",
      "train Loss: 75.7366\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 129/149\n",
      "----------\n",
      "train Loss: 74.0191\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 130/149\n",
      "----------\n",
      "train Loss: 78.3673\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 131/149\n",
      "----------\n",
      "train Loss: 75.1847\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 132/149\n",
      "----------\n",
      "train Loss: 78.1632\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 133/149\n",
      "----------\n",
      "train Loss: 74.7980\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 134/149\n",
      "----------\n",
      "train Loss: 74.1882\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 135/149\n",
      "----------\n",
      "train Loss: 76.0823\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 136/149\n",
      "----------\n",
      "train Loss: 76.6534\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 137/149\n",
      "----------\n",
      "train Loss: 76.9177\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 138/149\n",
      "----------\n",
      "train Loss: 75.0664\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 139/149\n",
      "----------\n",
      "train Loss: 77.9023\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 140/149\n",
      "----------\n",
      "train Loss: 76.8991\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 141/149\n",
      "----------\n",
      "train Loss: 77.6838\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 142/149\n",
      "----------\n",
      "train Loss: 75.2157\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 143/149\n",
      "----------\n",
      "train Loss: 75.5815\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 144/149\n",
      "----------\n",
      "train Loss: 75.6213\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 145/149\n",
      "----------\n",
      "train Loss: 75.7921\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 146/149\n",
      "----------\n",
      "train Loss: 75.7239\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 147/149\n",
      "----------\n",
      "train Loss: 75.1686\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 148/149\n",
      "----------\n",
      "train Loss: 77.5811\n",
      "val Loss: 26.0756\n",
      "\n",
      "Epoch 149/149\n",
      "----------\n",
      "train Loss: 76.4194\n",
      "val Loss: 26.0756\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best loss: 26.056634\n"
     ]
    }
   ],
   "source": [
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "    dataloaders = {'train':train_iter, 'val':test_iter}\n",
    "\n",
    "    dataset_sizes = {'train':len(train_iter.dataset),'val':len(test_iter.dataset)}\n",
    "    \n",
    "    model = AutoRecItemBased(50,num_users,dropout=0.01).to(mps_device)\n",
    "    \n",
    "    learning_rate = 5e-2\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    # Decay LR by a factor of 1e-5 every 10 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=1e-5)\n",
    "    model = train_model_item_based(model,dataloaders,dataset_sizes,mps_device,optimizer,exp_lr_scheduler,num_epochs=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference testing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user interaction matrix is a 2D matrix where rows represent items and columns represent users and the value of each element is the ranking.\n",
      "\n",
      "user-item interaction matrix training set shape: (1682, 943)\n",
      "user-item interaction matrix training set:\n",
      " [[5. 4. 0. ... 5. 0. 0.]\n",
      " [3. 0. 0. ... 0. 0. 5.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "First row from all interaction matrix:\n",
      " [5. 4. 0. 0. 4. 4. 0. 0. 0. 4. 0. 0. 3. 0. 1. 5. 4. 5. 0. 3.]\n",
      "First row from train interaction matrix:\n",
      " [0. 4. 0. 0. 4. 4. 0. 0. 0. 4. 0. 0. 3. 0. 1. 5. 4. 5. 0. 3.]\n",
      "First row from test interaction matrix:\n",
      " [5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "users, items, _, all_inter_mat = load_data_ml100k(data, num_users,num_items)\n",
    "print(\"user interaction matrix is a 2D matrix where rows represent items and columns represent users and the value of each element is the ranking.\")\n",
    "print()\n",
    "print(\"user-item interaction matrix training set shape:\",all_inter_mat.shape)\n",
    "print(\"user-item interaction matrix training set:\\n\",all_inter_mat)\n",
    "print()\n",
    "print(\"First row from all interaction matrix:\\n\",all_inter_mat[0][0:20])\n",
    "print(\"First row from train interaction matrix:\\n\",train_inter_mat[0][0:20])\n",
    "print(\"First row from test interaction matrix:\\n\",test_inter_mat[0][0:20])\n",
    "# Convert to PyTorch tensors\n",
    "all_inter_mat_tensor = torch.tensor(all_inter_mat, dtype=torch.float32,device=mps_device)\n",
    "actual_rating = all_inter_mat_tensor.flatten().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predicted_rating = np.array([])\n",
    "for user_item_inter_mat in all_inter_mat_tensor:\n",
    "    with torch.no_grad():\n",
    "        mask = torch.sign(user_item_inter_mat)\n",
    "        output = model(user_item_inter_mat)\n",
    "        masked_output = mask * output\n",
    "        predicted_rating = np.append(predicted_rating,masked_output.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual rating:\n",
      " [5. 4. 0. 0. 4. 4. 0. 0. 0. 4. 0. 0. 3. 0. 1. 5. 4. 5. 0. 3. 5. 0. 5. 0.\n",
      " 5. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 4. 5. 5. 4. 5. 0. 0. 0.\n",
      " 2. 0.]\n",
      "Predicted rating:\n",
      " [7. 6. 0. 0. 6. 6. 0. 0. 0. 8. 0. 0. 7. 0. 5. 7. 5. 6. 0. 4. 6. 0. 7. 0.\n",
      " 7. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 6. 6. 7. 7. 6. 0. 0. 0.\n",
      " 5. 0.]\n"
     ]
    }
   ],
   "source": [
    "predicted_rating_rounded = np.round(predicted_rating)\n",
    "print(\"Actual rating:\\n\", actual_rating[:50])\n",
    "print(\"Predicted rating:\\n\", predicted_rating_rounded[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct rating prediction: 27824\n",
      "Percentage of correct prediction: 1.7542112039018338\n"
     ]
    }
   ],
   "source": [
    "number_of_correct_prediction = np.array([1 if pred_rat==actual_rat and actual_rat!=0 else 0 for  pred_rat,actual_rat in zip(predicted_rating_rounded,actual_rating)]).sum()\n",
    "print(\"Number of correct rating prediction:\", number_of_correct_prediction)\n",
    "print(\"Percentage of correct prediction:\", number_of_correct_prediction/len(actual_rating)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration how to evaluate prediction in inference analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n",
      "tensor([[5., 0., 3.],\n",
      "        [0., 4., 0.],\n",
      "        [2., 0., 0.]])\n",
      "Prediction:\n",
      "tensor([[4.8000, 2.1000, 3.2000],\n",
      "        [1.5000, 3.9000, 2.0000],\n",
      "        [2.1000, 1.0000, 0.5000]])\n",
      "mask\n",
      "tensor([[1., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "Masked prediction\n",
      "tensor([[4.8000, 0.0000, 3.2000],\n",
      "        [0.0000, 3.9000, 0.0000],\n",
      "        [2.1000, 0.0000, 0.0000]])\n",
      "RMSE: 0.1581\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "test_data = np.array([\n",
    "    [5, 0, 3],\n",
    "    [0, 4, 0],\n",
    "    [2, 0, 0]\n",
    "])\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "print(\"Test data:\")\n",
    "print(test_data)\n",
    "\n",
    "# Model's predictions\n",
    "predictions = np.array([\n",
    "    [4.8, 2.1, 3.2],\n",
    "    [1.5, 3.9, 2.0],\n",
    "    [2.1, 1.0, 0.5]\n",
    "])\n",
    "predictions = torch.tensor(predictions, dtype=torch.float32)\n",
    "print(\"Prediction:\")\n",
    "print(predictions)\n",
    "\n",
    "# Mask using torch.sign\n",
    "mask = torch.sign(test_data)\n",
    "print(\"mask\")\n",
    "print(mask)\n",
    "\n",
    "# Masked predictions\n",
    "masked_predictions = predictions * mask\n",
    "print(\"Masked prediction\")\n",
    "print(masked_predictions)\n",
    "\n",
    "# Calculate RMSE only for observed entries\n",
    "mse_loss = nn.MSELoss(reduction='sum')\n",
    "squared_diff = mse_loss(test_data * mask, masked_predictions)\n",
    "rmse = torch.sqrt(squared_diff / torch.sum(mask))\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[1,2,3]\n",
    "list2=[1,2,3]\n",
    "\n",
    "a = [(i, j) for i, j in zip(list1, list2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 2), (3, 3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
